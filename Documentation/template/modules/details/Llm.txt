
## For the Application Developer ##

Plang LLM service proxies the OpenAI service. We charge 2x the price, you can see prices at https://openai.com/pricing, example gpt4 input price is 0.03, but using plang as proxy is 0.06.
supported models: gpt-4-1106-preview, gpt-4-1106-vision-preview, gpt-4, gpt-4-32k, gpt-3.5-turbo-1106, gpt-3.5-turbo-instruct
See how to use OpenAI service instead for plang service(/PlangOrOpenAI.md)

Llm request are cached locally, the cached response is store in .db/system.sqlite at the Operating system user data location, 
    On windows it is at C:\Users\[Username]\AppData\Roaming\plang\.db\system.sqlite
    On Linux it is at /usr/local/share/plang/.db/system.sqlite
    Macos it is at ~/Library/Application Support/plang/.db/system.sqlite
Important Note: the Linux and Macos have not been validated.

build steps are cached, only way to get llm to ask request new data is to modify the step or to delete cached result in db

.db/system.sqlite can become big because of caching, system does not clean up automatically.

Link to https://github.com/PLangHQ/plang/blob/main/Documentation/SupportedAI.md for list of supported models. 
## For the Application Developer ##
